{
  "metadata": {
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.15",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 2,
        "name": "ipython"
      }
    }
  },
  "nbformat_minor": 2,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Extracting and geolocating reports from Trove\nThis notebook walks through how I have been able to download report records from Trove and then use my custom geolocating scripts to geolocate report titles that have street addresses in them.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## 1. Extract the reports from Trove using its API\n\nTrove has a useful API that allows you to download data from queries in JSON and XML formats. You can then manipulate this data using other tools, including simple python scripts (as this project does). For further information about the Trove API see the official overview (https://help.nla.gov.au/trove/building-with-trove/api) and Tim Sherratt's very helpful *Glam Workbench* (https://glam-workbench.github.io/trove/).",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### 1.1 Set-up API extractor module\nThis is a module (reusable piece of code) that I had made to extract data from the JSON files provided by the Trove API.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import json, urllib2, csv, os, sys,re\nreload(sys)\nsys.setdefaultencoding('utf-8')\n\ndef PageSearcher(URL,exportFile):\n    #set up csv\n    existingList = open(exportFile, 'a')\n    writer = csv.writer(existingList, delimiter=\",\")\n    #set up JSON file from Trove API\n    response = urllib2.urlopen(URL)\n    responseJSON = response.read()\n    parsed_json = json.loads(responseJSON)\n    #save a copy of the response\n    filename = 'Trove-Archaeology-Reports_'+parsed_json['response']['zone'][0]['records']['s']+'.json'\n    with open(filename, 'w') as outfile:\n        json.dump(responseJSON, outfile)\n    #determine next URL\n    try:\n        NextURL = 'https://api.trove.nla.gov.au'+parsed_json['response']['zone'][0]['records']['next']+'&key=9rpf896ac4jvee00'\n    except:\n        NextURL = 'http://bom.gov.au'\n    #isolate list of works\n    works = parsed_json['response']['zone'][0]['records']['work']\n    #extract data for each work\n\n    for work in works:\n        TroveID = work['id']\n        troveUrl =work['troveUrl'].encode('utf8')\n        title = work['title'].encode('ascii','ignore')\n        try:\n            contributor = work['contributor'][0].encode('ascii','ignore')\n        except:\n            contributor = 'not found'\n        try:\n            issued = work['issued']\n        except:\n            issued = 'not found'\n        yearRE = re.search(r'((19|20)\\d\\d)',str(issued))\n        if yearRE is not None:\n            year = yearRE.group().strip()\n        else:\n            year = issued\n        try:\n            abstract = work['abstract'][0].encode('ascii','ignore')\n        except:\n            abstract = 'not found'\n        try:\n            subjects = str(work['subject']).encode('ascii','ignore')\n        except:\n            subjects = 'not found'\n        try:\n            URLs = str(work['identifier']).encode('utf8')\n        except:\n            URLs = 'not found'\n        try:\n            ExternalURL_1 = work['identifier'][0]['value'].encode('utf8')\n        except:\n            ExternalURL_1 = 'not found'\n        try:\n            ExternalURL_1_type = work['identifier'][0]['linktype'].encode('utf8')\n        except:\n            ExternalURL_1_type = 'not found'\n        try:\n            ExternalURL_2 = work['identifier'][1]['value'].encode('utf8')\n        except:\n            ExternalURL_2 = 'not found'\n        try:\n            ExternalURL_2_type = work['identifier'][1]['linktype'].encode('utf8')\n        except:\n            ExternalURL_2_type = 'not found'\n        relevanceScore = work['relevance']['score']\n        #Save to file\n        writer.writerow([TroveID,troveUrl,title,contributor,issued,year,abstract,subjects,URLs,ExternalURL_1,ExternalURL_1_type,ExternalURL_2,ExternalURL_2_type,relevanceScore])\n        #print the rows as the script works to show that it's working\n        print [TroveID,troveUrl,title,contributor,issued,year,abstract,subjects,URLs,subjects,URLs,ExternalURL_1,ExternalURL_1_type,ExternalURL_2,ExternalURL_2_type,relevanceScore]\n    existingList.close()\n    return(NextURL)",
      "metadata": {
        "trusted": true
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "I then run a short script to search the Trove API for all 'books' that are 'Australia content' with titles that contain either 'archaeological' or 'archaeology' AND either 'report', 'assessment', 'excavation' or 'investigation'. \n\nThis is equivalent to the boolean search: \"title:((archaeology OR archaeological) (report OR assessment OR excavation OR investigation))\" or the Trove URL: https://trove.nla.gov.au/book/result?q=title%3A((archaeology+OR+archaeological)+(report+OR+assessment+OR+excavation+OR+investigation))&l-australian=y",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#Define URL to search\nURL = 'http://api.trove.nla.gov.au/result?key=9rpf896ac4jvee00&q=title%3A((archaeology+OR+archaeological)+(report+OR+assessment+OR+excavation+OR+investigation))&l-australian=y&zone=book&encoding=json&include=links&reclevel=full&n=100&bulkHarvest=true'\n\n# number of pages to go through\nAttempts = 13\n\n# Name export file\nexportFile = 'Trove-Archaeology-Reports.csv'\n\n#Create export file\nNewList = open(exportFile, 'wb')\n\n#Make headings\nwriter = csv.writer(NewList, delimiter=\",\")\nheadings = [\"TroveID\",\"troveUrl\",\"title\",\"contributor\",\"issued\",\"year\",\"abstract\",\"subjects\",\"URLs\",\"ExternalURL_1\",\"ExternalURL_1_type\",\"ExternalURL_2\",\"ExternalURL_2_type\",\"relevanceScore\"]\nwriter.writerow(headings)\n\n#close export file\nNewList.close()\n\nAttempt = 0\nNextURL = URL\nwhile Attempt <= Attempts:\n    URL = NextURL\n    NextURL = PageSearcher(URL,exportFile)\n    print(NextURL)\n    Attempt = Attempt + 1",
      "metadata": {
        "trusted": true
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "This short piece of code shows us the first five reports listed in the CSV file.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\ndf = pd.read_csv('Trove-Archaeology-Reports.csv')\ndf.head(5) #shows first five reports listed",
      "metadata": {
        "trusted": true
      },
      "execution_count": 13,
      "outputs": [
        {
          "execution_count": 13,
          "output_type": "execute_result",
          "data": {
            "text/plain": "    TroveID                               troveUrl  \\\n0  24427856  http://trove.nla.gov.au/work/24427856   \n1   9545095   http://trove.nla.gov.au/work/9545095   \n2  25710955  http://trove.nla.gov.au/work/25710955   \n3  14335630  http://trove.nla.gov.au/work/14335630   \n4  24155453  http://trove.nla.gov.au/work/24155453   \n\n                                               title  \\\n0  Wybalenna: the archaeology of cultural accommo...   \n1  Sites of special scientific interest in the Vi...   \n2  Industrial and historical archaeology : papers...   \n3  Site surveys and significance assessment in Au...   \n4  Historical archaeological sites : investigatio...   \n\n                                         contributor     issued  year  \\\n0                                   Birmingham, Judy       1992  1992   \n1                     Coutts, P. J. F. (Peter J. F.)  1976-1978  1976   \n2              Australian Council of National Trusts       1981  1981   \n3  Springwood Conference on Australian Prehistory...  1984-1986  1984   \n4            New South Wales. Department of Planning       1993  1993   \n\n    abstract   subjects       URLs ExternalURL_1 ExternalURL_1_type  \\\n0  not found  not found  not found     not found          not found   \n1  not found  not found  not found     not found          not found   \n2  not found  not found  not found     not found          not found   \n3  not found  not found  not found     not found          not found   \n4  not found  not found  not found     not found          not found   \n\n  ExternalURL_2 ExternalURL_2_type  relevanceScore  \n0     not found          not found        0.499458  \n1     not found          not found        0.433081  \n2     not found          not found        0.358837  \n3     not found          not found        0.340455  \n4     not found          not found        0.273874  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>TroveID</th>\n      <th>troveUrl</th>\n      <th>title</th>\n      <th>contributor</th>\n      <th>issued</th>\n      <th>year</th>\n      <th>abstract</th>\n      <th>subjects</th>\n      <th>URLs</th>\n      <th>ExternalURL_1</th>\n      <th>ExternalURL_1_type</th>\n      <th>ExternalURL_2</th>\n      <th>ExternalURL_2_type</th>\n      <th>relevanceScore</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>24427856</td>\n      <td>http://trove.nla.gov.au/work/24427856</td>\n      <td>Wybalenna: the archaeology of cultural accommo...</td>\n      <td>Birmingham, Judy</td>\n      <td>1992</td>\n      <td>1992</td>\n      <td>not found</td>\n      <td>not found</td>\n      <td>not found</td>\n      <td>not found</td>\n      <td>not found</td>\n      <td>not found</td>\n      <td>not found</td>\n      <td>0.499458</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>9545095</td>\n      <td>http://trove.nla.gov.au/work/9545095</td>\n      <td>Sites of special scientific interest in the Vi...</td>\n      <td>Coutts, P. J. F. (Peter J. F.)</td>\n      <td>1976-1978</td>\n      <td>1976</td>\n      <td>not found</td>\n      <td>not found</td>\n      <td>not found</td>\n      <td>not found</td>\n      <td>not found</td>\n      <td>not found</td>\n      <td>not found</td>\n      <td>0.433081</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>25710955</td>\n      <td>http://trove.nla.gov.au/work/25710955</td>\n      <td>Industrial and historical archaeology : papers...</td>\n      <td>Australian Council of National Trusts</td>\n      <td>1981</td>\n      <td>1981</td>\n      <td>not found</td>\n      <td>not found</td>\n      <td>not found</td>\n      <td>not found</td>\n      <td>not found</td>\n      <td>not found</td>\n      <td>not found</td>\n      <td>0.358837</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>14335630</td>\n      <td>http://trove.nla.gov.au/work/14335630</td>\n      <td>Site surveys and significance assessment in Au...</td>\n      <td>Springwood Conference on Australian Prehistory...</td>\n      <td>1984-1986</td>\n      <td>1984</td>\n      <td>not found</td>\n      <td>not found</td>\n      <td>not found</td>\n      <td>not found</td>\n      <td>not found</td>\n      <td>not found</td>\n      <td>not found</td>\n      <td>0.340455</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>24155453</td>\n      <td>http://trove.nla.gov.au/work/24155453</td>\n      <td>Historical archaeological sites : investigatio...</td>\n      <td>New South Wales. Department of Planning</td>\n      <td>1993</td>\n      <td>1993</td>\n      <td>not found</td>\n      <td>not found</td>\n      <td>not found</td>\n      <td>not found</td>\n      <td>not found</td>\n      <td>not found</td>\n      <td>not found</td>\n      <td>0.273874</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "## 2. Search each report for possible addresses\nThis uses the naive address parser that I developped for Archaeology Near Me. The initial inspiration came from parts of the address parser https://github.com/SwoopSearch/pyaddress/tree/master/address, but the Archaeology Near Me address parser is much simpler, more naive, and adapted for finding Australian street addresses in strings of text which also contain other information.\n\nIts basic premise is to look for addresses in the format:\n\n    [Street number], Street Name, Street Type, Suburb/Locality, State Postcode.\n    \nIt first cleans up the text slightly removing spaces on either side of hyphens, and most punctuation. It then uses regular expressions to find the address.  Then the actual parsing takes place, working backwards through the address.\n\nThe identified address is then geolocated using either the NSW Address Location Web Service, provided by the NSW State Department of Finance (http://maps.six.nsw.gov.au/services/public/Address_Location), or the Google Maps API. I chose these services since their terms and conditions did not restrict the reuse of the geolocated results.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import re\nimport csv\nimport os\nimport urllib2\nimport json\n\ndef AddressFinder(AddressString):\n\n    #create simple list of localities in NSW based on ABS data: ASGS, Volume 3 - Non ABS Structures, July 2016  (http://www.abs.gov.au/AUSSTATS/abs@.nsf/DetailsPage/1270.0.55.003July%202016?OpenDocument)\n    LocalitiesList = []\n    LocListFile = open('BackgroundData/SSC_2016_NSW_mod.csv', 'rU')\n    LocList = csv.reader(LocListFile)\n\n    for row in LocList:\n        LN = row[0].rstrip()\n        LocalitiesList.append(LN)\n\n    #start off not knowing what the correct is\n    geoJSON = 'not found'\n    correct_suburb = 'not known'\n    longitude = 'not known'\n    latitude = 'not known'\n    VerifiedAddress = 'not verified'\n    AddressExport = 'No Street Address'\n    LocConf = 'unknown'\n\n    #Status Update\n    print \"Starting New Address\"\n    \n    #state Google Maps Geocoding API key\n    GoogleMapsGeocodeAPI_key = 'AIzaSyAHPwfJzNo5pS9P7-tGBfB6vKslhVZyQX4'\n\n    #create a list of street types and abbreviations\n    StreetTypes = []\n    StreetListFile = open('BackgroundData/StreetTypes.csv', 'rU')\n    StreetList = csv.reader(StreetListFile)\n    for row in StreetList:\n        LN = row[0].rstrip()\n        StreetTypes.append(LN)\n\n    #Clean up the string containing the address\n    AddressString = AddressString.replace('\\n',' ')  #remove new lines\n    AddressString = AddressString.replace(',', '')   #remove commas\n    AddressString = AddressString.replace(';', '')   #remove semiconons\n    AddressString = AddressString.replace('.', '')   #remove periods (aka 'full-stops')\n    AddressString = AddressString.replace('  ', ' ')  #remove double spaces\n    AddressString = AddressString.replace(' - ','-') #remove spaces around hyphens\n    AddressString = AddressString.strip().upper()    #convert to upper case and remove preceeding and trailing spaces\n    AddressString = AddressString.replace('PARAMATTA','PARRAMATTA') #fix a common spelling mistake/typo which is important for archaeology!\n    AddressString = AddressString.replace('REFERN','REDFERN') #fix another spelling mistake/typo - there must be a better way to address this issue.\n    AddressString = re.sub(r'((STAGE)\\s?(\\d*))', ' ', AddressString)\n\n    LocStart = 0\n    AddressLocality = ''\n\n    #Recognise addresses using a regular expression\n    AddressMatch = re.search(r'((\\d+)(-?)(\\d*)\\w?)\\s(O?\\'?)(\\w+)\\s([A-Z]+)',AddressString) #recognises Street addresses with O' - eg O'Connell Street.\n\n    #return function if no address is found\n    if AddressMatch == None:\n        #return ('No Street Address', 'not known', 'not known', 'not found')\n        print \"No Identified Address\"\n        return (AddressExport, VerifiedAddress, longitude, latitude, LocConf, geoJSON)\n    else:\n        StreetAddress = AddressMatch.group()\n        print StreetAddress\n\n    start = (AddressString.find(StreetAddress)) + len(StreetAddress)\n    end = len(AddressString)\n    EndAddressString = AddressString[start:end]\n\n    #Find a list of possible suburbs/localities in the address string.\n    suburb = []\n    StartA = -1\n    for p in LocalitiesList:\n        StartA = EndAddressString.find(p)\n        if StartA != -1:\n            print p\n            suburb.append(p)\n    #break address into components\n    AddressParts = StreetAddress.split()\n    StreetNumber = AddressParts[0]\n    StreetName = AddressParts[1]\n    StreetType = AddressParts[2]\n\n    #check that StreetType is actually a street type\n    StreetCheck = 'false'\n    for st in StreetTypes:\n        if StreetType == st:\n            StreetCheck = 'true'\n\n    #for cases where StreetType is not a street type, run another regex to find three part street names, eg 244 Old Northern Road, Castle Hill\n    if StreetCheck == 'false':\n        AddressMatch = re.search(r'((\\d+)(-?)(\\d*)\\w?)\\s(O?\\'?)(\\w+)\\s(O?\\'?)(\\w+)\\s([A-Z]+)',AddressString) #recognises Street addresses with O' - eg O'Connell Street.\n        if AddressMatch != None:\n            StreetAddress = AddressMatch.group()\n\n        #break address into components\n        AddressParts = StreetAddress.split()\n        #check that the last part of AddressParts is a street type\n        for st in StreetTypes:\n            if len(AddressParts) == 4 and AddressParts[3] == st:\n                StreetCheck = 'true'\n                StreetNumber = AddressParts[0]\n                StreetName = AddressParts[1] + '%20' + AddressParts[2]\n                StreetType = AddressParts[3]\n                break\n\n        #if the last part of AddressParts is not a street type, assume that the street name is non-standard and starts with 'The', eg 19 The Corso, Manly\n        if AddressParts[1] == 'THE':\n            StreetNumber = AddressParts[0]\n            StreetName = AddressParts[1] + '%20' + AddressParts[2]\n            StreetType = ''\n            StreetCheck = 'true'\n\n    #end the script here if there are no addresses in the format Number X Street, Number X Y Street, or Number The Z - this is to prevent nonsense results from Google's geocoder.\n    if StreetCheck == 'false':\n        AddressExport = StreetNumber + ' ' + StreetName + ' ' + StreetType + ', ' + correct_suburb + ', NSW'\n        AddressExport = AddressExport.replace(\"%20\", \" \")\n        return (AddressExport, VerifiedAddress, longitude, latitude, LocConf, geoJSON)\n\n    #determine if the street number is odd or even\n    OddEven = 'unknown'\n    NumberMatch = re.search(r'(\\d+)', StreetNumber)\n    FirstNumber = int(NumberMatch.group())\n    if (FirstNumber % 2 == 0):\n        OddEven = 'even' #even\n    else:\n        OddEven = 'odd'#odd\n\n\n    #Construct URL for access NSW LPI Address Location Web Service API (http://maps.six.nsw.gov.au/sws/AddressLocation.html) and use this to\n    for i in suburb:\n        i = i.replace(\" \",\"%20\")\n        URL = \"http://maps.six.nsw.gov.au/services/public/Address_Location?houseNumber=\"+StreetNumber+\"&roadName=\"+StreetName+\"&roadType=\"+StreetType+\"&suburb=\"+i+\"&postCode=&projection=EPSG%3A4326\"\n        response = urllib2.urlopen(URL)\n        responseJSON = response.read()\n        parsed_json = json.loads(responseJSON)\n        if parsed_json['addressResult']['addresses'] is not None:\n            print \"Found Address using SIX Address Parser\"\n            longitude = parsed_json['addressResult']['addresses'][0]['addressPoint']['centreX']\n            latitude = parsed_json['addressResult']['addresses'][0]['addressPoint']['centreY']\n            VerifiedAddress = parsed_json['addressResult']['addresses'][0]['shortAddressString'] + ', NSW ' + str(parsed_json['addressResult']['addresses'][0]['postCode']) + ', AUSTRALIA'\n            #set confidence level\n            methodDescriptions = str(parsed_json['addressResult']['searchMethod']['methodDescriptions'])\n            StartA = methodDescriptions.find('Address Returned')\n            if StartA != -1:\n                LocConf = 'Great - SIX'\n            else:\n                LocConf = parsed_json['addressResult']['searchMethod']['methodDescriptions']\n            #test if the given street number and the SIX reported street number are both odd or both even (ie, they are on the same side of the road, even if they are slightly different)\n            SIXFirstNumber = int(parsed_json['addressResult']['addresses'][0]['houseNumberFirst'])\n            if (SIXFirstNumber % 2 == 0):\n                SIXOddEven = 'even' #even\n            else:\n                SIXOddEven = 'odd'#odd\n            if SIXOddEven != OddEven:\n                LocConf = 'Odd-Even'\n            #also set LocConf to 'Poor' if the difference between the given street number and the SIX reported street number is greater than 5\n            diffAddressNum = SIXFirstNumber - FirstNumber\n            if abs(diffAddressNum) > 5:\n                LocConf = 'Mismatch'\n            # geoJSON = '{\\n\"type\": \"Feature\", \"geometry\": {\\n\"type\": \"Point\", \"coordinates\": ['+str(longitude)+', '+str(latitude)+']\\n}, \"properties\": {\\n\"name\": \"'+AddressString+'\", \"VerifiedAddress\": \"'+ VerifiedAddress +'\", \"GeolocationMethod\": \"LPI_Address\", \"GeolocationConfindence\": \"'+LocConf+'}}'\n            correct_suburb = i.replace(\"%20\", \" \")\n            break\n\n#     #for remaining cases still remaining, try the Google API (note - this has a limited number of returns per day, and other limitations such as you are required to charge no fees, attribution etc, see: https://developers.google.com/maps/terms)\n    if geoJSON == 'not found' or LocConf == 'Poor':\n        suburb.append(\"Sydney\") #there's a decent chance that the 'suburb' is the Sydney CBD if there is no suburb given\n        for i in suburb:\n            i = i.replace(\" \",\"%20\")\n            testAddress = str(StreetNumber) + ' ' + StreetName + ' ' + StreetType + ', ' + i + ', NSW'\n            testAddress = testAddress.replace(\" \",\"%20\")\n            URL = \"https://maps.googleapis.com/maps/api/geocode/json?key=\" + GoogleMapsGeocodeAPI_key + \"&new_forward_geocoder=true&region=au&address=\" + testAddress\n            response = urllib2.urlopen(URL)\n            responseJSON = response.read()\n            parsed_json = json.loads(responseJSON)\n            if parsed_json['status'] == 'OK':\n                print \"Found Address using Google\"\n                longitude = parsed_json['results'][0]['geometry']['location']['lng']\n                latitude = parsed_json['results'][0]['geometry']['location']['lat']\n                VerifiedAddress = parsed_json['results'][0]['formatted_address']\n                #set confidence level\n                if parsed_json['results'][0]['geometry']['location_type'] == 'ROOFTOP':\n                    LocConf = 'Great - Google'\n                    correct_suburb = i\n                    break\n                else:\n                    LocConf = parsed_json['results'][0]['geometry']['location_type']\n                geoJSON = '{\\n\"type\": \"Feature\", \"geometry\": {\\n\"type\": \"Point\", \"coordinates\": ['+str(longitude)+', '+str(latitude)+']}, \"properties\": {\\n\"name\": \"'+AddressString+'\", \"VerifiedAddress\": \"'+ VerifiedAddress +'\", \"GeolocationMethod\": \"Google_Address\", \"GeolocationConfindence\": \"'+LocConf+'}}'\n    \n\n    #return Addresses\n\n\n    #f = open('OutputData3.geojson','w')\n    #f.write(geoJSON)\n    #f.close()\n    AddressExport = StreetNumber + ' ' + StreetName + ' ' + StreetType + ', ' + correct_suburb + ', NSW'\n    AddressExport = AddressExport.replace(\"%20\", \" \")\n    if correct_suburb == 'not known':\n        correct_suburb = suburb\n\n\n    return (AddressExport, VerifiedAddress, longitude, latitude, LocConf)\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "I then the address locator module on the list of reports from Trove. (At this point I need to admit that I had to download the file 'Trove-Archaeology-Reports.csv', correct the last row using Microsoft Excel, and then upload it again, to prevent a read error that was stopping this script from working.)",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import csv, os\n\n#open existing list of reports and create a new list of reports\n\nReports = open('Trove-Archaeology-Reports.csv', 'rU')\nNewList = open('Trove-Archaeology-Reports_located.csv', 'wb')\n\n#create a dictionary from the open reports\nOpenReports = csv.DictReader(Reports)\n\n#Make headings \nwriter = csv.writer(NewList, delimiter=\",\")\nheadings = [\"TroveID\",\"troveUrl\",\"title\",\"contributor\",\"issued\",\"year\",\"abstract\",\"subjects\",\"URLs\",\"ExternalURL_1\",\"ExternalURL_1_type\",\"relevanceScore\",\"Address_export\",\"Verified_Address\",\"Longitude\",\"Latitude\",\"Location_Confidence\"]\nwriter.writerow(headings)\n\nfor report in OpenReports:\n    SearchResults = AddressFinder(report[\"title\"])\n       \n    row = [report[\"TroveID\"],report[\"troveUrl\"],report[\"title\"],report[\"contributor\"],report[\"issued\"],report[\"year\"],report[\"abstract\"],report[\"subjects\"],report[\"URLs\"],report[\"ExternalURL_1\"],report[\"ExternalURL_1_type\"],report[\"relevanceScore\"],SearchResults[0],SearchResults[1],SearchResults[2],SearchResults[3],SearchResults[4]]\n    writer.writerow(row)\n    print row\n    print 'written to CSV - now for the next record!'\n\n#close export file\nNewList.close()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "This short code shows the first five rows of the geolocated reports.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df = pd.read_csv('Trove-Archaeology-Reports_located.csv',sep=',',engine='python')\nselectValues = ['RANGE_INTERPOLATED','Great - Google']\nLocation_Confidence_test = df.Location_Confidence.isin(selectValues)\ndf_locConf = df[Location_Confidence_test] #see https://cmdlinetips.com/2018/02/how-to-subset-pandas-dataframe-based-on-values-of-a-column/\ndf_locConf.filter(items=[\"TroveID\",\"troveUrl\",\"title\",\"contributor\",\"year\",\"Address_export\",\"Verified_Address\",\"Longitude\",\"Latitude\",\"Location_Confidence\"]).head()  #see http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.filter.html#pandas.DataFrame.filter\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": 151,
      "outputs": [
        {
          "execution_count": 151,
          "output_type": "execute_result",
          "data": {
            "text/plain": "     TroveID                                troveUrl  \\\n4  182505676  http://trove.nla.gov.au/work/182505676   \n5  171902379  http://trove.nla.gov.au/work/171902379   \n6   36993128   http://trove.nla.gov.au/work/36993128   \n7  178266696  http://trove.nla.gov.au/work/178266696   \n8   33530106   http://trove.nla.gov.au/work/33530106   \n\n                                               title  \\\n4  Final report : Division 1 Boggo Road Gaol, Bri...   \n5  Permit application : archaeological assessment...   \n6  Air raid shelter, 30 Tamar Street, Ballina. Ex...   \n7  Archaeological Assessment, 15a Dickson Street,...   \n8  Archaeological assessment of proposed developm...   \n\n                        contributor  year  \\\n4               Austral Archaeology  2007   \n5                 Atkinson, Fenella  2007   \n6                Resitech Australia  2009   \n7  Archaeology And Heritage Pty Ltd  2003   \n8                      Gojak, Denis  2002   \n\n                         Address_export  \\\n4            1 BOGGO ROAD, AUSTRAL, NSW   \n5  9 RANGIHOU CRESCENT, PARRAMATTA, NSW   \n6         30 TAMAR STREET, BALLINA, NSW   \n7      15A DICKSON STREET, NEWTOWN, NSW   \n8     197 DENISON STREET, HAMILTON, NSW   \n\n                                  Verified_Address    Longitude     Latitude  \\\n4     21 Boggo Rd, Dutton Park QLD 4102, Australia  153.0288235  -27.4951182   \n5  9 Rangihou Cres, Parramatta NSW 2150, Australia  151.0149799  -33.8143619   \n6         30 Tamar St, Ballina NSW 2478, Australia   153.565107  -28.8714617   \n7      15A Dickson St, Newtown NSW 2042, Australia  151.1793767  -33.9046149   \n8     197 Denison St, Hamilton NSW 2303, Australia  151.7406453  -32.9254861   \n\n  Location_Confidence  \n4      Great - Google  \n5      Great - Google  \n6      Great - Google  \n7      Great - Google  \n8      Great - Google  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>TroveID</th>\n      <th>troveUrl</th>\n      <th>title</th>\n      <th>contributor</th>\n      <th>year</th>\n      <th>Address_export</th>\n      <th>Verified_Address</th>\n      <th>Longitude</th>\n      <th>Latitude</th>\n      <th>Location_Confidence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>4</th>\n      <td>182505676</td>\n      <td>http://trove.nla.gov.au/work/182505676</td>\n      <td>Final report : Division 1 Boggo Road Gaol, Bri...</td>\n      <td>Austral Archaeology</td>\n      <td>2007</td>\n      <td>1 BOGGO ROAD, AUSTRAL, NSW</td>\n      <td>21 Boggo Rd, Dutton Park QLD 4102, Australia</td>\n      <td>153.0288235</td>\n      <td>-27.4951182</td>\n      <td>Great - Google</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>171902379</td>\n      <td>http://trove.nla.gov.au/work/171902379</td>\n      <td>Permit application : archaeological assessment...</td>\n      <td>Atkinson, Fenella</td>\n      <td>2007</td>\n      <td>9 RANGIHOU CRESCENT, PARRAMATTA, NSW</td>\n      <td>9 Rangihou Cres, Parramatta NSW 2150, Australia</td>\n      <td>151.0149799</td>\n      <td>-33.8143619</td>\n      <td>Great - Google</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>36993128</td>\n      <td>http://trove.nla.gov.au/work/36993128</td>\n      <td>Air raid shelter, 30 Tamar Street, Ballina. Ex...</td>\n      <td>Resitech Australia</td>\n      <td>2009</td>\n      <td>30 TAMAR STREET, BALLINA, NSW</td>\n      <td>30 Tamar St, Ballina NSW 2478, Australia</td>\n      <td>153.565107</td>\n      <td>-28.8714617</td>\n      <td>Great - Google</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>178266696</td>\n      <td>http://trove.nla.gov.au/work/178266696</td>\n      <td>Archaeological Assessment, 15a Dickson Street,...</td>\n      <td>Archaeology And Heritage Pty Ltd</td>\n      <td>2003</td>\n      <td>15A DICKSON STREET, NEWTOWN, NSW</td>\n      <td>15A Dickson St, Newtown NSW 2042, Australia</td>\n      <td>151.1793767</td>\n      <td>-33.9046149</td>\n      <td>Great - Google</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>33530106</td>\n      <td>http://trove.nla.gov.au/work/33530106</td>\n      <td>Archaeological assessment of proposed developm...</td>\n      <td>Gojak, Denis</td>\n      <td>2002</td>\n      <td>197 DENISON STREET, HAMILTON, NSW</td>\n      <td>197 Denison St, Hamilton NSW 2303, Australia</td>\n      <td>151.7406453</td>\n      <td>-32.9254861</td>\n      <td>Great - Google</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "## 3. Search for Heritage Item names\nThis next step used a version of the address finder to search for heritage item names for those report titles that did not contain street addresses (as searched for in step 2.)\n\nThe first heritage list search was for items listed on the NSW State Heritage Register. Common words such as 'shop', 'house', 'terraces' and so forth, were removed from the list before searching.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#A naive attempt at searching for Heritage Listed sites in long strings of information\n\nimport re\nimport csv\nimport os\n\ndef HeritageSiteFinder(AddressString):\n\n    #create simple list of Heritage Sites\n    SitesList = []\n    HeritageListFile = open('BackgroundData/SHR-sites-list-cleaned.csv', 'rU')\n    HeritageList = csv.DictReader(HeritageListFile)\n\n\n    #Status Update\n    print \"Starting New Site\"\n\n    #Clean up the string containing the address\n    AddressString = AddressString.replace('\\n',' ')  #remove new lines\n    AddressString = AddressString.replace(',', '')   #remove commas\n    AddressString = AddressString.replace(';', '')   #remove semiconons\n    AddressString = AddressString.replace('.', '')   #remove periods (aka 'full-stops')\n    AddressString = AddressString.replace('  ', ' ')  #remove double spaces\n    AddressString = AddressString.replace(' - ','-') #remove spaces around hyphens\n    AddressString = AddressString.strip().upper()    #convert to upper case and remove preceeding and trailing spaces\n    AddressString = AddressString.replace('PARAMATTA','PARRAMATTA') #fix a common spelling mistake/typo which is important for archaeology!\n    AddressString = AddressString.replace('REFERN','REDFERN') #fix another spelling mistake/typo - there must be a better way to address this issue.\n\n    Results = []\n    for row in HeritageList:\n        ITEMNAME = row['ITEMNAME'].strip().upper().replace('.', '')\n        ITEMNAME = ITEMNAME.replace(' GROUP','')\n        HOITEMID = row['HOITEMID']\n        export = (HOITEMID,row['ITEMNAME'])\n        rp = re.compile(ITEMNAME)\n        result = rp.search(AddressString)\n        if result != None:\n            result1 = result.group()\n            Results.append(export)\n            \n    if Results == None:\n        Results.append('none found')\n    \n \n    print AddressString\n    print Results\n    return (Results)\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "This piece of code then runs the finder for SHR sites.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import csv, os\n\n#open existing list of reports and create a new list of reports\n\nReports = open('Trove-Archaeology-Reports_located.csv', 'rU')\nNewList = open('Trove-Archaeology-Reports_locatedv2.csv', 'wb')\n\n#create a dictionary from the open reports\nOpenReports = csv.DictReader(Reports)\n\n#Make headings \nwriter = csv.writer(NewList, delimiter=\",\")\nheadings = [\"TroveID\",\"troveUrl\",\"title\",\"contributor\",\"issued\",\"year\",\"abstract\",\"subjects\",\"URLs\",\"ExternalURL_1\",\"ExternalURL_1_type\",\"relevanceScore\",\"Address_export\",\"Verified_Address\",\"Longitude\",\"Latitude\",\"Location_Confidence\",\"HOITEMID\",\"ITEMNAME\",\"SHR-Search-Results\"]\nwriter.writerow(headings)\n\nfor report in OpenReports:\n    if report[\"Location_Confidence\"] == 'unknown':\n        SearchResults = HeritageSiteFinder(report[\"title\"])\n\n        row = [report[\"TroveID\"],report[\"troveUrl\"],report[\"title\"],report[\"contributor\"],report[\"issued\"],report[\"year\"],report[\"abstract\"],report[\"subjects\"],report[\"URLs\"],report[\"ExternalURL_1\"],report[\"ExternalURL_1_type\"],report[\"relevanceScore\"],report[\"Address_export\"],report[\"Verified_Address\"],report[\"Longitude\"],report[\"Latitude\"],report[\"Location_Confidence\"],SearchResults,SearchResults,'test']\n    else:\n        row = [report[\"TroveID\"],report[\"troveUrl\"],report[\"title\"],report[\"contributor\"],report[\"issued\"],report[\"year\"],report[\"abstract\"],report[\"subjects\"],report[\"URLs\"],report[\"ExternalURL_1\"],report[\"ExternalURL_1_type\"],report[\"relevanceScore\"],report[\"Address_export\"],report[\"Verified_Address\"],report[\"Longitude\"],report[\"Latitude\"],report[\"Location_Confidence\"],'n/a','n/a','n/a']\n    writer.writerow(row)\n    \n    print row\n    print 'written to CSV - now for the next record!'\n#close export file\nNewList.close()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "The resulting list was then cleaned up to check for obvious false positives. One report that referenced two sites (Hyde Park Barracks and First Government House) was duplicated on the list, so that it could be linked to both sites. The number of reports matched to SHR sites was rather small, with only 43 reports matched to SHR sites.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\ndf = pd.read_csv('BackgroundData/Trove-Archaeology-Reports_located_SHR.csv')\ndf.filter(items=[\"TroveID\",\"troveUrl\",\"title\",\"contributor\",\"year\",\"HOITEMID\",\"ITEMNAME\"]).head(7)",
      "metadata": {
        "trusted": true
      },
      "execution_count": 6,
      "outputs": [
        {
          "execution_count": 6,
          "output_type": "execute_result",
          "data": {
            "text/plain": "      TroveID                               troveUrl  \\\n0  22002787.0  http://trove.nla.gov.au/work/22002787   \n1  26410915.0  http://trove.nla.gov.au/work/26410915   \n2  34152896.0  http://trove.nla.gov.au/work/34152896   \n3  27781950.0  http://trove.nla.gov.au/work/27781950   \n4  34907222.0  http://trove.nla.gov.au/work/34907222   \n5  31991723.0  http://trove.nla.gov.au/work/31991723   \n6  10383154.0  http://trove.nla.gov.au/work/10383154   \n\n                                               title  \\\n0  Archaeological investigation of the out buildi...   \n1  Report on the archaeological investigation of ...   \n2  The conservation of the gardens and grounds of...   \n3  Frederick Ash building conservation plan and C...   \n4  Frederick Ash building conservation plan and C...   \n5  Critique, the public archaeological process : ...   \n6  Assessment of historical and archaeological re...   \n\n                 contributor    year   HOITEMID                ITEMNAME  \n0  Higginbotham, Edward A. K  1990.0  5045008.0          Vaucluse House  \n1  Higginbotham, Edward A. K  1999.0  5045008.0          Vaucluse House  \n2  Higginbotham, Edward A. K  1984.0  5045008.0          Vaucluse House  \n3             Rankine & Hill  1994.0  5045387.0  Frederick Ash Building  \n4             Rankine & Hill  1992.0  5045387.0  Frederick Ash Building  \n5               Thorp, Wendy  1994.0  5060189.0               Hyde Park  \n6               Crook, Penny  2003.0  5001030.0          Susannah Place  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>TroveID</th>\n      <th>troveUrl</th>\n      <th>title</th>\n      <th>contributor</th>\n      <th>year</th>\n      <th>HOITEMID</th>\n      <th>ITEMNAME</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>22002787.0</td>\n      <td>http://trove.nla.gov.au/work/22002787</td>\n      <td>Archaeological investigation of the out buildi...</td>\n      <td>Higginbotham, Edward A. K</td>\n      <td>1990.0</td>\n      <td>5045008.0</td>\n      <td>Vaucluse House</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>26410915.0</td>\n      <td>http://trove.nla.gov.au/work/26410915</td>\n      <td>Report on the archaeological investigation of ...</td>\n      <td>Higginbotham, Edward A. K</td>\n      <td>1999.0</td>\n      <td>5045008.0</td>\n      <td>Vaucluse House</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>34152896.0</td>\n      <td>http://trove.nla.gov.au/work/34152896</td>\n      <td>The conservation of the gardens and grounds of...</td>\n      <td>Higginbotham, Edward A. K</td>\n      <td>1984.0</td>\n      <td>5045008.0</td>\n      <td>Vaucluse House</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>27781950.0</td>\n      <td>http://trove.nla.gov.au/work/27781950</td>\n      <td>Frederick Ash building conservation plan and C...</td>\n      <td>Rankine &amp; Hill</td>\n      <td>1994.0</td>\n      <td>5045387.0</td>\n      <td>Frederick Ash Building</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>34907222.0</td>\n      <td>http://trove.nla.gov.au/work/34907222</td>\n      <td>Frederick Ash building conservation plan and C...</td>\n      <td>Rankine &amp; Hill</td>\n      <td>1992.0</td>\n      <td>5045387.0</td>\n      <td>Frederick Ash Building</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>31991723.0</td>\n      <td>http://trove.nla.gov.au/work/31991723</td>\n      <td>Critique, the public archaeological process : ...</td>\n      <td>Thorp, Wendy</td>\n      <td>1994.0</td>\n      <td>5060189.0</td>\n      <td>Hyde Park</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>10383154.0</td>\n      <td>http://trove.nla.gov.au/work/10383154</td>\n      <td>Assessment of historical and archaeological re...</td>\n      <td>Crook, Penny</td>\n      <td>2003.0</td>\n      <td>5001030.0</td>\n      <td>Susannah Place</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "## 4. Manual processing for sites in Sydney and Parramatta\nMy PhD project is only interested in sites in the County of Cumberland, roughly equivalent to the Sydney Basin. Therefore, I want to make a shorter list of reports with titles that contain Sydney suburb names.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import csv, re, os\ndef SuburbFinder(AddressString):\n\n    #create simple list of Heritage Sites\n    SitesList = []\n    SuburbListFile = open('BackgroundData/List_Suburbs_Cumberland_County.csv', 'rU')\n    SuburbList = csv.DictReader(SuburbListFile)\n    #Status Update\n    print (\"Starting New Site: \"+ AddressString)\n\n    #Clean up the string containing the address\n    AddressString = AddressString.replace('\\n',' ')  #remove new lines\n    AddressString = AddressString.replace(',', '')   #remove commas\n    AddressString = AddressString.replace(';', '')   #remove semiconons\n    AddressString = AddressString.replace('.', '')   #remove periods (aka 'full-stops')\n    AddressString = AddressString.replace('  ', ' ')  #remove double spaces\n    AddressString = AddressString.replace(' - ','-') #remove spaces around hyphens\n    AddressString = AddressString.strip().upper()    #convert to upper case and remove preceeding and trailing spaces\n    AddressString = AddressString.replace('PARAMATTA','PARRAMATTA') #fix a common spelling mistake/typo which is important for archaeology!\n    AddressString = AddressString.replace('REFERN','REDFERN') #fix another spelling mistake/typo - there must be a better way to address this issue.\n\n    Results = []\n    for row in SuburbList:\n        rp = re.compile(row['LOCALITY_NAME'])\n        result = rp.search(AddressString)\n        if result != None:\n            result1 = result.group()\n            Results.append(row['LOCALITY_NAME'])\n            \n    if Results == None:\n        Results.append('none found')\n\n    return (Results)\n\nReports = open('BackgroundData/Trove-Archaeology-Reports_to_locate.csv', 'rU')\nNewList = open('Results/Trove-Archaeology-Reports_suburbs.csv', 'wb')\n\n#create a dictionary from the open reports\nOpenReports = csv.DictReader(Reports)\n\n#Make headings \nwriter = csv.writer(NewList, delimiter=\",\")\nheadings = [\"TroveID\",\"troveUrl\",\"title\",\"contributor\",\"issued\",\"year\",\"abstract\",\"subjects\",\"URLs\",\"ExternalURL_1\",\"ExternalURL_1_type\",\"relevanceScore\",\"suburbs\"]\nwriter.writerow(headings)\n\nfor report in OpenReports:\n    SearchResults = SuburbFinder(report[\"title\"])\n    row = [report[\"TroveID\"],report[\"troveUrl\"],report[\"title\"],report[\"contributor\"],report[\"issued\"],report[\"year\"],report[\"abstract\"],report[\"subjects\"],report[\"URLs\"],report[\"ExternalURL_1\"],report[\"ExternalURL_1_type\"],report[\"relevanceScore\"],SearchResults]\n    \n    writer.writerow(row)\n\n#close export file\nNewList.close()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "This search resulted in 244 reports with Sydney suburb names in them, once the obvious false positives of 'Austral' were removed. This formed a manageable list that I could then manually go through and create a field with the 'site name'. These site names could refer to the historical site, eg 'Native Institution, Blacktown' or to the present geography, eg 'Green Road Reserve, Kellyville'. After this manual sorting process, I had a list of 140 reports associated with 110 places.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "I then wanted to see if I could try the same on a list of 1125 reports from NSW Archaeology Online that I had not previously been able to locate.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import csv, re, os\ndef SuburbFinder(AddressString):\n\n    #create simple list of Heritage Sites\n    SitesList = []\n    SuburbListFile = open('BackgroundData/List_Suburbs_Cumberland_County.csv', 'rU')\n    SuburbList = csv.DictReader(SuburbListFile)\n    #Status Update\n    print (\"Starting New Site: \"+ AddressString)\n\n    #Clean up the string containing the address\n    AddressString = AddressString.replace('\\n',' ')  #remove new lines\n    AddressString = AddressString.replace(',', '')   #remove commas\n    AddressString = AddressString.replace(';', '')   #remove semiconons\n    AddressString = AddressString.replace('.', '')   #remove periods (aka 'full-stops')\n    AddressString = AddressString.replace('  ', ' ')  #remove double spaces\n    AddressString = AddressString.replace(' - ','-') #remove spaces around hyphens\n    AddressString = AddressString.strip().upper()    #convert to upper case and remove preceeding and trailing spaces\n    AddressString = AddressString.replace('PARAMATTA','PARRAMATTA') #fix a common spelling mistake/typo which is important for archaeology!\n    AddressString = AddressString.replace('REFERN','REDFERN') #fix another spelling mistake/typo - there must be a better way to address this issue.\n\n    Results = []\n    for row in SuburbList:\n        rp = re.compile(row['LOCALITY_NAME'])\n        result = rp.search(AddressString)\n        if result != None:\n            result1 = result.group()\n            Results.append(row['LOCALITY_NAME'])\n            \n    if Results == None:\n        Results.append('none found')\n\n    return (Results)\n\nReports = open('BackgroundData/NSW_AOL_Reports_Not-located.csv', 'rU')\nNewList = open('Results/NSW_AOL_Reports_Suburbs.csv', 'wb')\n\n#create a dictionary from the open reports\nOpenReports = csv.DictReader(Reports)\n\n#Make headings \nwriter = csv.writer(NewList, delimiter=\",\")\nheadings = [\"ID\",\"Title\",\"Author\",\"Organisation\",\"Client\",\"Year\",\"item_link\",\"Document Type\",\"NSW Historic Theme\",\"Original Document Quality\",\"Contributor\",\"Site Location\",\"County\",\"LGA\",\"Address (export)\",\"Verified Address\",\"Longitude\",\"Latitude\",\"Location Confidence\",\"Suburb\"]\nwriter.writerow(headings)\n\nfor report in OpenReports:\n    SearchResults = SuburbFinder(report[\"Title\"])\n    row = [report[\"ID\"],report[\"Title\"],report[\"Author\"],report[\"Organisation\"],report[\"Client\"],report[\"Year\"],report[\"item_link\"],report[\"Document Type\"],report[\"NSW Historic Theme\"],report[\"Original Document Quality\"],report[\"Contributor\"],report[\"Site Location\"],report[\"County\"],report[\"LGA\"],report[\"Address (export)\"],report[\"Verified Address\"],report[\"Longitude\"],report[\"Latitude\"],report[\"Location Confidence\"],SearchResults]\n    \n    writer.writerow(row)\n\n#close export file\nNewList.close()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "This resulted in a list of 369 places. I then thought that I should search the list for matches to my existing list of sites, so that I did not have to identify the same site names twice.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import csv, re, os\ndef SuburbFinder(AddressString):\n\n    #create simple list of Heritage Sites\n    SitesList = []\n    SuburbListFile = open('BackgroundData/Trove-Archaeology-Reports_named-sites.csv', 'rU')\n    SuburbList = csv.DictReader(SuburbListFile)\n    #Status Update\n#     print (\"Starting New Site: \"+ AddressString)\n\n    #Clean up the string containing the address\n    AddressString = AddressString.replace('\\n',' ')  #remove new lines\n    AddressString = AddressString.replace(',', '')   #remove commas\n    AddressString = AddressString.replace(';', '')   #remove semiconons\n    AddressString = AddressString.replace('.', '')   #remove periods (aka 'full-stops')\n    AddressString = AddressString.replace('  ', ' ')  #remove double spaces\n    AddressString = AddressString.replace(' - ','-') #remove spaces around hyphens\n    AddressString = AddressString.strip().upper()    #convert to upper case and remove preceeding and trailing spaces\n    AddressString = AddressString.replace('PARAMATTA','PARRAMATTA') #fix a common spelling mistake/typo which is important for archaeology!\n    AddressString = AddressString.replace('REFERN','REDFERN') #fix another spelling mistake/typo - there must be a better way to address this issue.\n\n    Results = []\n    for row in SuburbList:\n        Short_site_name = row['Short_site_name'].strip().upper()\n        rp = re.compile(Short_site_name)\n        result = rp.search(AddressString)\n        if result != None:\n            result1 = result.group()\n            Results.append([row['site_name'],row['Short_site_name']])\n            \n    if Results == []:\n        Results.append(['none found','none found'])\n\n    return (Results)\n\nReports = open('Results/NSW_AOL_Reports_Suburbs.csv', 'rU')\nNewList = open('Results/NSW_AOL_Reports_Suburbs_sites.csv', 'wb')\n\n#create a dictionary from the open reports\nOpenReports = csv.DictReader(Reports)\n\n#Make headings \nwriter = csv.writer(NewList, delimiter=\",\")\nheadings = [\"ID\",\"Title\",\"site_name\",\"Short_site_name\",\"Author\",\"Organisation\",\"Client\",\"Year\",\"item_link\",\"Document Type\",\"NSW Historic Theme\",\"Original Document Quality\",\"Contributor\",\"Site Location\",\"County\",\"LGA\",\"Address (export)\",\"Verified Address\",\"Longitude\",\"Latitude\",\"Location Confidence\",\"Suburb\"]\nwriter.writerow(headings)\n\nfor report in OpenReports:\n    SearchResults = SuburbFinder(report[\"Title\"])\n    row = [report[\"ID\"],report[\"Title\"],SearchResults[0][0],SearchResults[0][1],report[\"Author\"],report[\"Organisation\"],report[\"Client\"],report[\"Year\"],report[\"item_link\"],report[\"Document Type\"],report[\"NSW Historic Theme\"],report[\"Original Document Quality\"],report[\"Contributor\"],report[\"Site Location\"],report[\"County\"],report[\"LGA\"],report[\"Address (export)\"],report[\"Verified Address\"],report[\"Longitude\"],report[\"Latitude\"],report[\"Location Confidence\"],report[\"Suburb\"]]\n    \n    writer.writerow(row)\n    print (SearchResults)\n\n#close export file\nNewList.close()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "After some manual processing, I had a list of 492 reports associated with 221 sites (I also reviewed the locations of sites in the County of Cumberland). When I combined these two lists, I had 322 distinct short site names. I then searched the list of reports that had already been located (reports_maps_release_4.csv) against the list of short site names.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import csv, re, os\ndef SuburbFinder(AddressString):\n\n    #create simple list of Heritage Sites\n    SitesList = []\n    SitesListFile = open('BackgroundData/Site_names_list.csv', 'rU')\n    SitesList = csv.DictReader(SitesListFile)\n    #Status Update\n#     print (\"Starting New Site: \"+ AddressString)\n\n    #Clean up the string containing the address\n    AddressString = AddressString.replace('\\n',' ')  #remove new lines\n    AddressString = AddressString.replace(',', '')   #remove commas\n    AddressString = AddressString.replace(';', '')   #remove semiconons\n    AddressString = AddressString.replace('.', '')   #remove periods (aka 'full-stops')\n    AddressString = AddressString.replace('  ', ' ')  #remove double spaces\n    AddressString = AddressString.replace(' - ','-') #remove spaces around hyphens\n    AddressString = AddressString.strip().upper()    #convert to upper case and remove preceeding and trailing spaces\n    AddressString = AddressString.replace('PARAMATTA','PARRAMATTA') #fix a common spelling mistake/typo which is important for archaeology!\n    AddressString = AddressString.replace('REFERN','REDFERN') #fix another spelling mistake/typo - there must be a better way to address this issue.\n\n    Results = []\n    for row in SitesList:\n        Short_site_name = row['Short_site_name'].strip().upper()\n        rp = re.compile(Short_site_name)\n        result = rp.search(AddressString)\n        if result != None:\n            result1 = result.group()\n            Results.append([row['site_name'],row['Short_site_name']])\n            \n    if Results == []:\n        Results.append(['none found','none found'])\n\n    return (Results)\n\nReports = open('BackgroundData/reports_maps_release_4.csv', 'rU')\nNewList = open('Results/named-located-sites.csv', 'wb')\n\n#create a dictionary from the open reports\nOpenReports = csv.DictReader(Reports)\n\n#Make headings \nwriter = csv.writer(NewList, delimiter=\",\")\nheadings = [\"project_id\",\"Located_Title\",\"site_name\",\"Short_site_name\",\"X\",\"Y\",\"results\"]\nwriter.writerow(headings)\n\nfor report in OpenReports:\n    SearchResults = SuburbFinder(report[\"title\"])\n    row = [report[\"project_id\"],report[\"title\"],SearchResults[0][0],SearchResults[0][1],report[\"X\"],report[\"Y\"],SearchResults]\n    \n    writer.writerow(row)\n    print (SearchResults)\n\n#close export file\nNewList.close()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "This process was quite successful - out of the 322 site names, 112 could be geolocated by matching them to report titles that already had been located.\nI then tried to geolocate the remaining 210 sites using the Google Maps Geocoder.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import csv, re, os, urllib2, json\n\ndef SiteFinder(AddressString):\n    #state Google Maps Geocoding API key\n    GoogleMapsGeocodeAPI_key = 'AIzaSyAHPwfJzNo5pS9P7-tGBfB6vKslhVZyQX4'\n    \n    #Clean up the string containing the address\n    AddressString = AddressString.replace('\\n',' ')  #remove new lines\n    AddressString = AddressString.replace(',', '')   #remove commas\n    AddressString = AddressString.replace(';', '')   #remove semiconons\n    AddressString = AddressString.replace('.', '')   #remove periods (aka 'full-stops')\n    AddressString = AddressString.replace('  ', ' ')  #remove double spaces\n    AddressString = AddressString.replace(' - ','-') #remove spaces around hyphens\n    AddressString = AddressString.strip().upper()    #convert to upper case and remove preceeding and trailing spaces\n    AddressString = AddressString.replace('PARAMATTA','PARRAMATTA') #fix a common spelling mistake/typo which is important for archaeology!\n    AddressString = AddressString.replace('REFERN','REDFERN') #fix another spelling mistake/typo - there must be a better way to address this issue.\n    \n    testAddress = AddressString+ ', NSW'\n    testAddress = testAddress.replace(\" \",\"%20\")\n    URL = \"https://maps.googleapis.com/maps/api/geocode/json?key=\" + GoogleMapsGeocodeAPI_key + \"&new_forward_geocoder=true&region=au&address=\" + testAddress\n    response = urllib2.urlopen(URL)\n    responseJSON = response.read()\n    parsed_json = json.loads(responseJSON)\n    if parsed_json['status'] == 'OK':\n        print \"Found Address using Google\"\n        longitude = parsed_json['results'][0]['geometry']['location']['lng']\n        latitude = parsed_json['results'][0]['geometry']['location']['lat']\n        VerifiedAddress = parsed_json['results'][0]['formatted_address'].encode('utf-8', errors='ignore')\n        #set confidence level\n        if parsed_json['results'][0]['geometry']['location_type'] == 'ROOFTOP':\n            LocConf = 'Great - Google'\n        else:\n            LocConf = parsed_json['results'][0]['geometry']['location_type'].encode('utf-8', errors='ignore')\n    else:\n        VerifiedAddress = 'not found'\n        longitude = 'unknown'\n        latitude = 'unknown'\n        LocConf = 'unknown'\n        \n    return (VerifiedAddress, longitude, latitude, LocConf)\n\nReports = open('BackgroundData/site_names_to_search.csv', 'rU')\nNewList = open('Results/site_names_searched.csv', 'wb')\n\n#create a dictionary from the open reports\nOpenReports = csv.DictReader(Reports)\n\n#Make headings \nwriter = csv.writer(NewList, delimiter=\",\")\nheadings = [\"site_name\",\"Short_site_name\",\"VerifiedAddress\",\"X\",\"Y\",\"LocConf\",\"results\"]\nwriter.writerow(headings)\n\nfor report in OpenReports:\n    SearchResults = SiteFinder(report[\"site_name\"])\n    row = [report[\"site_name\"],report[\"Short_site_name\"],SearchResults[0],SearchResults[1],SearchResults[2],SearchResults[3],SearchResults]\n    \n    writer.writerow(row)\n    print (SearchResults)\n\n#close export file\nNewList.close()\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "The geocoder was quite successful - although I needed to check the results and remove locations that were wrong, even though the Geocoder 'confidence level' was quite high - for example it located *'public domain \"under the freeway\" at Bulwarra Rd & Harris St., Ultimo'* as *'1 Barnhill Rd, Terrigal NSW 2260, Australia'*. However once I had removed these mistakes I still had 174 new site locations. I added a further 28 sites manually as part of the correction process.\n\nI can then use this list to geocode the lists of reports I identified earlier.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import csv, re, os\ndef SuburbFinder(AddressString):\n\n    #create simple list of Heritage Sites\n    SitesList = []\n    SitesListFile = open('BackgroundData/named-located-sites.csv', 'rU')\n    SitesList = csv.DictReader(SitesListFile)\n    #Status Update\n#     print (\"Starting New Site: \"+ AddressString)\n\n    #Clean up the string containing the address\n    AddressString = AddressString.replace('\\n',' ')  #remove new lines\n    AddressString = AddressString.replace(',', '')   #remove commas\n    AddressString = AddressString.replace(';', '')   #remove semiconons\n    AddressString = AddressString.replace('.', '')   #remove periods (aka 'full-stops')\n    AddressString = AddressString.replace('  ', ' ')  #remove double spaces\n    AddressString = AddressString.replace(' - ','-') #remove spaces around hyphens\n    AddressString = AddressString.strip().upper()    #convert to upper case and remove preceeding and trailing spaces\n    AddressString = AddressString.replace('PARAMATTA','PARRAMATTA') #fix a common spelling mistake/typo which is important for archaeology!\n    AddressString = AddressString.replace('REFERN','REDFERN') #fix another spelling mistake/typo - there must be a better way to address this issue.\n\n    Results = []\n    for row in SitesList:\n        site_name = row['site_name'].strip().upper()\n        rp = re.compile(site_name)\n        result = rp.search(AddressString)\n        if result != None:\n            result1 = result.group()\n            Results.append([row['X'],row['Y']])\n            \n    if Results == []:\n        Results.append(['none found','none found'])\n\n    return (Results)\n\nReports = open('BackgroundData/Trove-Archaeology-Reports_named-sites.csv', 'rU')\nNewList = open('Results/Trove-Archaeology-Reports_named-sites_located.csv', 'wb')\n\n#create a dictionary from the open reports\nOpenReports = csv.DictReader(Reports)\n\n#Make headings \nwriter = csv.writer(NewList, delimiter=\",\")\nheadings = [\"TroveID\",\"troveUrl\",\"title\",\"site_name\",\"contributor\",\"issued\",\"year\",\"abstract\",\"subjects\",\"URLs\",\"ExternalURL_1\",\"ExternalURL_1_type\",\"relevanceScore\",\"suburbs\",\"longitude\",\"latitude\"]\nwriter.writerow(headings)\n\nfor report in OpenReports:\n    SearchResults = SuburbFinder(report[\"site_name\"])\n    row = [report[\"TroveID\"],report[\"troveUrl\"],report[\"title\"],report[\"site_name\"],report[\"contributor\"],report[\"issued\"],report[\"year\"],report[\"abstract\"],report[\"subjects\"],report[\"URLs\"],report[\"ExternalURL_1\"],report[\"ExternalURL_1_type\"],report[\"relevanceScore\"],SearchResults[0][0],SearchResults[0][1]]\n    \n    writer.writerow(row)\n\n#close export file\nNewList.close()\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}